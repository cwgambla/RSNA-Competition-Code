{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7393fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Building the dataset...\n",
      "total_train_steps=449600\n",
      "warmup_steps=44960\n",
      "decay_steps=404640\n",
      "[INFO] Building the model...\n",
      "(32, 256, 256, 3)\n",
      "[INFO] Building the model...\n",
      "[INFO] Compiling the model...\n",
      "[INFO] Training...\n",
      "Epoch 1/50\n",
      "281/281 [==============================] - 31s 91ms/step - loss: 3.6688 - bowel_loss: 0.6995 - extra_loss: 0.6667 - liver_loss: 0.5796 - kidney_loss: 0.7500 - spleen_loss: 0.9729 - bowel_accuracy: 0.4884 - extra_accuracy: 0.6393 - liver_accuracy: 0.8421 - kidney_accuracy: 0.7494 - spleen_accuracy: 0.5737 - val_loss: 3.4866 - val_bowel_loss: 0.6931 - val_extra_loss: 0.6592 - val_liver_loss: 0.6409 - val_kidney_loss: 0.5878 - val_spleen_loss: 0.9055 - val_bowel_accuracy: 0.4814 - val_extra_accuracy: 0.6955 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8105 - val_spleen_accuracy: 0.6765\n",
      "Epoch 2/50\n",
      "281/281 [==============================] - 25s 90ms/step - loss: 3.6602 - bowel_loss: 0.6930 - extra_loss: 0.6566 - liver_loss: 0.6312 - kidney_loss: 0.7080 - spleen_loss: 0.9714 - bowel_accuracy: 0.4857 - extra_accuracy: 0.6428 - liver_accuracy: 0.8707 - kidney_accuracy: 0.7778 - spleen_accuracy: 0.6149 - val_loss: 3.3606 - val_bowel_loss: 0.6931 - val_extra_loss: 0.6495 - val_liver_loss: 0.5883 - val_kidney_loss: 0.5673 - val_spleen_loss: 0.8625 - val_bowel_accuracy: 0.4884 - val_extra_accuracy: 0.6955 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8105 - val_spleen_accuracy: 0.6765\n",
      "Epoch 3/50\n",
      "281/281 [==============================] - 25s 89ms/step - loss: 3.5878 - bowel_loss: 0.6932 - extra_loss: 0.6510 - liver_loss: 0.6067 - kidney_loss: 0.6945 - spleen_loss: 0.9424 - bowel_accuracy: 0.4973 - extra_accuracy: 0.6610 - liver_accuracy: 0.8710 - kidney_accuracy: 0.7820 - spleen_accuracy: 0.6094 - val_loss: 3.2810 - val_bowel_loss: 0.6930 - val_extra_loss: 0.6432 - val_liver_loss: 0.5533 - val_kidney_loss: 0.5558 - val_spleen_loss: 0.8357 - val_bowel_accuracy: 0.5186 - val_extra_accuracy: 0.6955 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8105 - val_spleen_accuracy: 0.6765\n",
      "Epoch 4/50\n",
      "281/281 [==============================] - 25s 89ms/step - loss: 3.5382 - bowel_loss: 0.6932 - extra_loss: 0.6425 - liver_loss: 0.5812 - kidney_loss: 0.6881 - spleen_loss: 0.9332 - bowel_accuracy: 0.4959 - extra_accuracy: 0.6689 - liver_accuracy: 0.8710 - kidney_accuracy: 0.7886 - spleen_accuracy: 0.6145 - val_loss: 3.2335 - val_bowel_loss: 0.6929 - val_extra_loss: 0.6369 - val_liver_loss: 0.5351 - val_kidney_loss: 0.5412 - val_spleen_loss: 0.8273 - val_bowel_accuracy: 0.5186 - val_extra_accuracy: 0.6955 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8105 - val_spleen_accuracy: 0.6765\n",
      "Epoch 5/50\n",
      "281/281 [==============================] - 26s 91ms/step - loss: 3.4785 - bowel_loss: 0.6935 - extra_loss: 0.6431 - liver_loss: 0.5662 - kidney_loss: 0.6717 - spleen_loss: 0.9039 - bowel_accuracy: 0.4897 - extra_accuracy: 0.6714 - liver_accuracy: 0.8709 - kidney_accuracy: 0.7919 - spleen_accuracy: 0.6200 - val_loss: 3.1639 - val_bowel_loss: 0.6928 - val_extra_loss: 0.6320 - val_liver_loss: 0.5069 - val_kidney_loss: 0.5283 - val_spleen_loss: 0.8040 - val_bowel_accuracy: 0.5186 - val_extra_accuracy: 0.6955 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8105 - val_spleen_accuracy: 0.6765\n",
      "Epoch 6/50\n",
      "281/281 [==============================] - 27s 94ms/step - loss: 3.4256 - bowel_loss: 0.6933 - extra_loss: 0.6352 - liver_loss: 0.5490 - kidney_loss: 0.6544 - spleen_loss: 0.8936 - bowel_accuracy: 0.4890 - extra_accuracy: 0.6841 - liver_accuracy: 0.8711 - kidney_accuracy: 0.7992 - spleen_accuracy: 0.6221 - val_loss: 3.1077 - val_bowel_loss: 0.6926 - val_extra_loss: 0.6231 - val_liver_loss: 0.4936 - val_kidney_loss: 0.5114 - val_spleen_loss: 0.7869 - val_bowel_accuracy: 0.5186 - val_extra_accuracy: 0.6955 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8105 - val_spleen_accuracy: 0.6838\n",
      "Epoch 7/50\n",
      "281/281 [==============================] - 28s 97ms/step - loss: 3.3586 - bowel_loss: 0.6932 - extra_loss: 0.6272 - liver_loss: 0.5338 - kidney_loss: 0.6299 - spleen_loss: 0.8745 - bowel_accuracy: 0.4948 - extra_accuracy: 0.6854 - liver_accuracy: 0.8709 - kidney_accuracy: 0.8060 - spleen_accuracy: 0.6416 - val_loss: 3.0324 - val_bowel_loss: 0.6925 - val_extra_loss: 0.6161 - val_liver_loss: 0.4755 - val_kidney_loss: 0.4947 - val_spleen_loss: 0.7536 - val_bowel_accuracy: 0.5186 - val_extra_accuracy: 0.6955 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8105 - val_spleen_accuracy: 0.6915\n",
      "Epoch 8/50\n",
      "281/281 [==============================] - 27s 96ms/step - loss: 3.3108 - bowel_loss: 0.6934 - extra_loss: 0.6196 - liver_loss: 0.5176 - kidney_loss: 0.6180 - spleen_loss: 0.8622 - bowel_accuracy: 0.4891 - extra_accuracy: 0.6946 - liver_accuracy: 0.8708 - kidney_accuracy: 0.8134 - spleen_accuracy: 0.6470 - val_loss: 2.9797 - val_bowel_loss: 0.6923 - val_extra_loss: 0.6063 - val_liver_loss: 0.4543 - val_kidney_loss: 0.4802 - val_spleen_loss: 0.7466 - val_bowel_accuracy: 0.5186 - val_extra_accuracy: 0.7088 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8152 - val_spleen_accuracy: 0.7045\n",
      "Epoch 9/50\n",
      "281/281 [==============================] - 27s 96ms/step - loss: 3.2402 - bowel_loss: 0.6932 - extra_loss: 0.6064 - liver_loss: 0.4966 - kidney_loss: 0.6072 - spleen_loss: 0.8368 - bowel_accuracy: 0.4998 - extra_accuracy: 0.7075 - liver_accuracy: 0.8709 - kidney_accuracy: 0.8185 - spleen_accuracy: 0.6504 - val_loss: 2.9217 - val_bowel_loss: 0.6921 - val_extra_loss: 0.5982 - val_liver_loss: 0.4397 - val_kidney_loss: 0.4659 - val_spleen_loss: 0.7258 - val_bowel_accuracy: 0.5186 - val_extra_accuracy: 0.7204 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8271 - val_spleen_accuracy: 0.7041\n",
      "Epoch 10/50\n",
      "281/281 [==============================] - 27s 96ms/step - loss: 3.1788 - bowel_loss: 0.6930 - extra_loss: 0.5998 - liver_loss: 0.4820 - kidney_loss: 0.5844 - spleen_loss: 0.8195 - bowel_accuracy: 0.4958 - extra_accuracy: 0.7163 - liver_accuracy: 0.8716 - kidney_accuracy: 0.8222 - spleen_accuracy: 0.6628 - val_loss: 2.8440 - val_bowel_loss: 0.6919 - val_extra_loss: 0.5808 - val_liver_loss: 0.4272 - val_kidney_loss: 0.4492 - val_spleen_loss: 0.6949 - val_bowel_accuracy: 0.5186 - val_extra_accuracy: 0.7417 - val_liver_accuracy: 0.8684 - val_kidney_accuracy: 0.8328 - val_spleen_accuracy: 0.7068\n",
      "Epoch 11/50\n",
      "159/281 [===============>..............] - ETA: 1:40 - loss: 3.4274 - bowel_loss: 0.6940 - extra_loss: 0.5186 - liver_loss: 0.6739 - kidney_loss: 0.6853 - spleen_loss: 0.8557 - bowel_accuracy: 0.4344 - extra_accuracy: 0.8200 - liver_accuracy: 0.7852 - kidney_accuracy: 0.7752 - spleen_accuracy: 0.6423"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 355\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 355\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# %% [markdown]\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# ## Visualize the training plots\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Create a 3x2 grid for the subplots\u001b[39;00m\n\u001b[0;32m    366\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m15\u001b[39m))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\Python_3.8_GPU\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "# You can use `tensorflow`, `pytorch`, `jax` here\n",
    "# KerasCore makes the notebook backend agnostic :)\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# %% [markdown]\n",
    "# # Configuration\n",
    "# \n",
    "# A particularly good practise is to have a configuration class for your notebooks. This not only keeps your configurations all at a single place but also becomes handy to map the configs to the performance of the model.\n",
    "# \n",
    "# Please play around with the configurations and see how the performance of the model changes.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Note on some observations\n",
    "# \n",
    "# Reference Notebook: https://www.kaggle.com/code/aritrag/eda-train-csv\n",
    "# \n",
    "# 1. Class Dependencies: Refers to inherent relationships between classes in the analysis.\n",
    "# 2. Complementarity: `bowel_injury` and `bowel_healthy`, as well as `extravasation_injury` and `extravasation_healthy`, are perfectly complementary, with their sum always equal to 1.0.\n",
    "# 3. Simplification: For the model, only `{bowel/extravasation}_injury` will be included, and the corresponding healthy status can be calculated using a sigmoid function.\n",
    "# 4. Softmax: `{kidney/liver/spleen}_{healthy/low/high}` classifications are softmaxed, ensuring their combined probabilities sum up to 1.0 for each organ, simplifying the model while preserving essential information.\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:23.231676Z\",\"iopub.execute_input\":\"2023-09-09T19:02:23.232042Z\",\"iopub.status.idle\":\"2023-09-09T19:02:23.247729Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:23.232007Z\",\"shell.execute_reply\":\"2023-09-09T19:02:23.246654Z\"}}\n",
    "class Config:\n",
    "    SEED = 42\n",
    "    IMAGE_SIZE = [224, 224]\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    TARGET_COLS  = [\n",
    "        \"bowel_injury\", \"extravasation_injury\",\n",
    "        \"kidney_healthy\", \"kidney_low\", \"kidney_high\",\n",
    "        \"liver_healthy\", \"liver_low\", \"liver_high\",\n",
    "        \"spleen_healthy\", \"spleen_low\", \"spleen_high\",\n",
    "    ]\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# %% [markdown]\n",
    "# # Reproducibility\n",
    "# \n",
    "# We would want this notebook to have reproducible results. Here we set the seed for all the random algorithms so that we can reproduce the experiments each time exactly the same way.\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:23.251220Z\",\"iopub.execute_input\":\"2023-09-09T19:02:23.251610Z\",\"iopub.status.idle\":\"2023-09-09T19:02:23.298498Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:23.251581Z\",\"shell.execute_reply\":\"2023-09-09T19:02:23.297532Z\"}}\n",
    "keras.utils.set_random_seed(seed=config.SEED)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Dataset\n",
    "# \n",
    "# The dataset provided in the competition consists of DICOM images. We will not be training on the DICOM images, rather would work on PNG image which are extracted from the DICOM format.\n",
    "# \n",
    "# [A helpful resource on the conversion of DICOM to PNG](https://www.kaggle.com/code/radek1/how-to-process-dicom-images-to-pngs)\n",
    "\n",
    "# %% [code] {\"_kg_hide-input\":true,\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:23.304536Z\",\"iopub.execute_input\":\"2023-09-09T19:02:23.304911Z\",\"iopub.status.idle\":\"2023-09-09T19:02:23.310205Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:23.304884Z\",\"shell.execute_reply\":\"2023-09-09T19:02:23.308909Z\"}}\n",
    "BASE_PATH = f\"D:/RSNA_Project/archive\"\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Meta Data\n",
    "# \n",
    "# The `train.csv` file contains the following meta information:\n",
    "# \n",
    "# - `patient_id`: A unique ID code for each patient.\n",
    "# - `series_id`: A unique ID code for each scan.\n",
    "# - `instance_number`: The image number within the scan. The lowest instance number for many series is above zero as the original scans were cropped to the abdomen.\n",
    "# - `[bowel/extravasation]_[healthy/injury]`: The two injury types with binary targets.\n",
    "# - `[kidney/liver/spleen]_[healthy/low/high]`: The three injury types with three target levels.\n",
    "# - `any_injury`: Whether the patient had any injury at all.\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:23.311914Z\",\"iopub.execute_input\":\"2023-09-09T19:02:23.313537Z\",\"iopub.status.idle\":\"2023-09-09T19:02:23.443167Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:23.313500Z\",\"shell.execute_reply\":\"2023-09-09T19:02:23.441731Z\"}}\n",
    "# train\n",
    "dataframe = pd.read_csv(f\"{BASE_PATH}/train.csv\")\n",
    "dataframe[\"image_path\"] = f\"{BASE_PATH}/train_images\"\\\n",
    "                    + \"/\" + dataframe.patient_id.astype(str)\\\n",
    "                    + \"/\" + dataframe.series_id.astype(str)\\\n",
    "                    + \"/\" + dataframe.instance_number.astype(str) +\".png\"\n",
    "dataframe = dataframe.drop_duplicates()\n",
    "\n",
    "dataframe.head(2)\n",
    "\n",
    "# %% [markdown]\n",
    "# We split the training dataset into train and validation. This is a common practise in the Machine Learning pipelines. We not only want to train our model, but also want to validate it's training.\n",
    "# \n",
    "# A small catch here is that the training and validation data should have an aligned data distribution. Here we handle that by grouping the lables and then splitting the dataset. This ensures an aligned data distribution between the training and the validation splits.\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:23.446378Z\",\"iopub.execute_input\":\"2023-09-09T19:02:23.446699Z\",\"iopub.status.idle\":\"2023-09-09T19:02:23.562238Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:23.446671Z\",\"shell.execute_reply\":\"2023-09-09T19:02:23.560740Z\"}}\n",
    "# Function to handle the split for each group\n",
    "def split_group(group, test_size=0.25):\n",
    "    if len(group) == 1:\n",
    "        return (group, pd.DataFrame()) if np.random.rand() < test_size else (pd.DataFrame(), group)\n",
    "    else:\n",
    "        return train_test_split(group, test_size=test_size, random_state=42)\n",
    "\n",
    "# Initialize the train and validation datasets\n",
    "train_data = pd.DataFrame()\n",
    "val_data = pd.DataFrame()\n",
    "\n",
    "# Iterate through the groups and split them, handling single-sample groups\n",
    "for _, group in dataframe.groupby(config.TARGET_COLS):\n",
    "    train_group, val_group = split_group(group)\n",
    "    train_data = pd.concat([train_data, train_group], ignore_index=True)\n",
    "    val_data = pd.concat([val_data, val_group], ignore_index=True)\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:23.564011Z\",\"iopub.execute_input\":\"2023-09-09T19:02:23.564429Z\",\"iopub.status.idle\":\"2023-09-09T19:02:23.572678Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:23.564388Z\",\"shell.execute_reply\":\"2023-09-09T19:02:23.571464Z\"}}\n",
    "train_data.shape, val_data.shape\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Data Pipeline /w tf.data\n",
    "# \n",
    "# Here we build the data pipeline using `tf.data`. Using `tf.data` we can map out data to an augmentation pipeline simple by using the ` map` API.\n",
    "# \n",
    "# Adding augmentations to the data pipeline is as simple as adding a layer into the list of layers that the `Augmenter` processes.\n",
    "# \n",
    "# Reference: https://keras.io/api/keras_cv/layers/augmentation/\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:23.574325Z\",\"iopub.execute_input\":\"2023-09-09T19:02:23.575215Z\",\"iopub.status.idle\":\"2023-09-09T19:02:23.586203Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:23.575161Z\",\"shell.execute_reply\":\"2023-09-09T19:02:23.585314Z\"}}\n",
    "def decode_image_and_label(image_path, label):\n",
    "    file_bytes = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_png(file_bytes, channels=3, dtype=tf.uint8)\n",
    "    image = tf.image.resize(image, config.IMAGE_SIZE, method=\"bilinear\")\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    label = tf.cast(label, tf.float32)\n",
    "    #         bowel       fluid       kidney      liver       spleen\n",
    "    labels = (label[0:1], label[1:2], label[2:5], label[5:8], label[8:11])\n",
    "    \n",
    "    return (image, labels)\n",
    "\n",
    "\n",
    "# def apply_augmentation(images, labels):\n",
    "#     augmenter = keras_cv.layers.Augmenter(\n",
    "#         [\n",
    "#             keras_cv.layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n",
    "#             keras_cv.layers.RandomCutout(height_factor=0.2, width_factor=0.2),\n",
    "            \n",
    "#         ]\n",
    "#     )\n",
    "#     return (augmenter(images), labels)\n",
    "\n",
    "\n",
    "def build_dataset(image_paths, labels):\n",
    "    num_validation_samples = len(image_paths)\n",
    "    \n",
    "    # Calculate the number of batches required for the given batch size\n",
    "    num_batches = num_validation_samples // config.BATCH_SIZE\n",
    "\n",
    "    # Calculate the new total number of samples after truncation\n",
    "    new_num_samples = (num_batches) * config.BATCH_SIZE\n",
    "    \n",
    "    # Truncate the validation data to the new size\n",
    "    image_paths = image_paths[:new_num_samples]\n",
    "    labels = labels[:new_num_samples]\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        .map(decode_image_and_label, num_parallel_calls=config.AUTOTUNE)\n",
    "        .shuffle(config.BATCH_SIZE * 10)\n",
    "        .batch(config.BATCH_SIZE)\n",
    "#         .map(apply_augmentation, num_parallel_calls=config.AUTOTUNE)\n",
    "        .prefetch(config.AUTOTUNE)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:23.587583Z\",\"iopub.execute_input\":\"2023-09-09T19:02:23.588024Z\",\"iopub.status.idle\":\"2023-09-09T19:02:24.523971Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:23.587992Z\",\"shell.execute_reply\":\"2023-09-09T19:02:24.522907Z\"}}\n",
    "paths  = train_data.image_path.tolist()\n",
    "labels = train_data[config.TARGET_COLS].values\n",
    "\n",
    "ds = build_dataset(image_paths=paths, labels=labels)\n",
    "images, labels = next(iter(ds))\n",
    "images.shape, [label.shape for label in labels]\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:24.525330Z\",\"iopub.execute_input\":\"2023-09-09T19:02:24.525691Z\",\"iopub.status.idle\":\"2023-09-09T19:02:24.895828Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:24.525657Z\",\"shell.execute_reply\":\"2023-09-09T19:02:24.894736Z\"}}\n",
    "# No more customizing your plots by hand, KerasCV has your back ;)\n",
    "\n",
    "# %% [markdown]\n",
    "# # Build Model\n",
    "# \n",
    "# We are going to load a pretrained model from the [list of avaiable backbones in KerasCV](https://keras.io/api/keras_cv/models/backbones/). We are using the `ResNetBackbone` as our backbone. The practise of using a pretrained model and finetuning it to a specific dataset is prevalent in the DL community.\n",
    "# \n",
    "# We use the [Functional API](https://keras.io/guides/functional_api/) of Keras to build the model. The design of the model would be such that we input a single image and we get different heads for the various predictions we need (kidney, spleen...).\n",
    "# \n",
    "# We have also added a Learning Rate scheduler for you to work with. When an athlete trains, the first step is always to warm up. We take a similar approach to training our models. We warm up with model where the learning rate increses from the initial LR to a higher LR. After the warmup stage we provide a decay algorithm (cosine here). A list of all the learning rate scheduler can be found [here](https://keras.io/api/optimizers/learning_rate_schedules/).\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:24.897740Z\",\"iopub.execute_input\":\"2023-09-09T19:02:24.898547Z\",\"iopub.status.idle\":\"2023-09-09T19:02:24.927962Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:24.898507Z\",\"shell.execute_reply\":\"2023-09-09T19:02:24.927231Z\"}}\n",
    "def build_model(warmup_steps, decay_steps):\n",
    "    # Define Input\n",
    "    inputs = keras.Input(shape=config.IMAGE_SIZE + [3,], batch_size=config.BATCH_SIZE,dtype='float32')\n",
    "    print(inputs.shape)\n",
    "    # Define Backbone\n",
    "#     backbone = keras_cv.models.ResNetBackbone.from_preset(\"resnet50_imagenet\")\n",
    "#     backbone.include_rescaling = False\n",
    "#     x = backbone(inputs)\n",
    "    x1 = keras.layers.Conv2D(8,(3,3),activation = 'relu',padding = 'same',dtype='float32')(inputs)\n",
    "    x1 = keras.layers.MaxPool2D((2,2),dtype='float32')(x1)\n",
    "    x1 = keras.layers.Conv2D(16,(3,3),activation = 'relu',padding = 'same',dtype='float32')(inputs)\n",
    "    x1 = keras.layers.MaxPool2D((2,2),dtype='float32')(x1)\n",
    "    x1 = keras.layers.Conv2D(32,(3,3),activation = 'relu',padding = 'same',dtype='float32')(x1)\n",
    "    x1 = keras.layers.MaxPool2D((2,2),dtype='float32')(x1)\n",
    "    x1 = keras.layers.Conv2D(64,(3,3),activation = 'relu',padding = 'same',dtype='float32')(x1)\n",
    "    x1 = keras.layers.MaxPool2D((2,2),dtype='float32')(x1)\n",
    "    x = keras.layers.Flatten(dtype='float32')(x1)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    # GAP to get the activation maps\n",
    "#     gap = keras.layers.GlobalAveragePooling2D()\n",
    "#     x = gap(x)\n",
    "\n",
    "    \n",
    "    #extraversion model\n",
    "    \n",
    "    x2 = keras.layers.Dense(128,activation= 'silu',dtype='float32')(x)\n",
    "    x2 = keras.layers.Dropout(0.5,dtype='float32')(x2)\n",
    "    x2 = keras.layers.Dense(64,activation= 'silu',dtype='float32')(x2)\n",
    "    x2 = keras.layers.Dropout(0.5,dtype='float32')(x2)\n",
    "    x2 = keras.layers.Dense(32,activation= 'silu',dtype='float32')(x2)\n",
    "    x2 = keras.layers.Dropout(0.5,dtype='float32')(x2)\n",
    "    x_extra = keras.layers.Dense(16, activation='silu',dtype='float32')(x2)\n",
    "    x_extra = keras.layers.Dropout(0.5,dtype='float32')(x_extra)\n",
    "    out_extra = keras.layers.Dense(1, name='extra', activation='sigmoid',dtype='float32')(x_extra) # use sigmoid to convert predictions to [0-1]\n",
    "    \n",
    "    #liver model\n",
    "    x_liver = keras.layers.Dense(16, activation='silu',dtype='float32')(x)\n",
    "    x_liver = keras.layers.Dropout(0.5,dtype='float32')(x_liver)\n",
    "    out_liver = keras.layers.Dense(3, name='liver', activation='softmax',dtype='float32')(x_liver) # use softmax for the liver head\n",
    "    \n",
    "    #kidney model\n",
    "    x_kidney = keras.layers.Dense(32, activation='silu',dtype='float32')(x)\n",
    "    x_kidney = keras.layers.Dropout(0.5,dtype='float32')(x_kidney)\n",
    "    x_kidney = keras.layers.Dense(16, activation='silu',dtype='float32')(x_kidney )\n",
    "    x_kidney = keras.layers.Dropout(0.5,dtype='float32')(x_kidney)\n",
    "    out_kidney = keras.layers.Dense(3, name='kidney', activation='softmax',dtype='float32')(x_kidney) # use softmax for the kidney head\n",
    "    \n",
    "    #spleen model\n",
    "    x_spleen = keras.layers.Dense(32,activation=\"relu\",dtype='float32')(x)\n",
    "    x_spleen = keras.layers.Dropout(0.5,dtype='float32')(x_spleen)\n",
    "    x_spleen = keras.layers.Dense(16, activation='silu',dtype='float32')(x_spleen)\n",
    "    x_spleen = keras.layers.Dropout(0.5,dtype='float32')(x_spleen)\n",
    "    out_spleen = keras.layers.Dense(3, name='spleen', activation='softmax',dtype='float32')(x_spleen) # use softmax for the spleen head\n",
    "    \n",
    "    #bowel model\n",
    "#     x_bowel = keras.layers.Conv2D(64,(3,3),activation = 'relu',padding = 'same',dtype='float64')(x1)\n",
    "#     x_bowel = keras.layers.MaxPool2D((2,2),dtype='float64')(x_bowel)\n",
    "#     x_bowel = keras.layers.Conv2D(64,(3,3),activation = 'relu',padding = 'same',dtype='float64')(x_bowel)\n",
    "#     x_bowel = keras.layers.MaxPool2D((2,2),dtype='float64')(x_bowel)\n",
    "#     x_bowel = keras.layers.Flatten(dtype='float64')(x_bowel)\n",
    "#     x_bowel = keras.layers.Dropout(0.5)(x_bowel)\n",
    "#     x_bowel = keras.layers.Dense(128,activation= 'relu',dtype='float64')(x_bowel)\n",
    "#     x_bowel = keras.layers.Dropout(0.5)(x_bowel)\n",
    "#     x_bowel = keras.layers.Dense(16, activation='relu')(x_bowel)\n",
    "#     x_bowel = keras.layers.Dropout(0.5)(x_bowel)\n",
    "#     backbone = keras_cv.models.ResNetBackbone.from_preset(\"resnet50_imagenet\")\n",
    "#     backbone.include_rescaling = False\n",
    "#     x_bowel = backbone(inputs)\n",
    "    x_bowel = keras.layers.Flatten(dtype='float32')(x1)\n",
    "    x_bowel = keras.layers.Dropout(0.5,dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dense(128,activation= 'silu',dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dropout(0.5,dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dense(64,activation= 'silu',dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dropout(0.5,dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dense(64,activation= 'silu',dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dropout(0.5,dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dense(32,activation= 'silu',dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dropout(0.5,dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dense(32,activation= 'silu',dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dropout(0.5,dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dense(32,activation= 'silu',dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dropout(0.5,dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dense(32,activation= 'silu',dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dropout(0.5,dtype='float32')(x_bowel)\n",
    "    x_bowel = keras.layers.Dense(16,activation= 'silu',dtype='float32')(x_bowel)\n",
    "#     x_bowel = keras.layers.Dropout(0.5,dtype='float32')(x_bowel)\n",
    "    \n",
    "    out_bowel = keras.layers.Dense(1, name='bowel', activation='sigmoid',dtype='float32')(x_bowel) # use sigmoid to convert predictions to [0-1]\n",
    "    \n",
    "    \n",
    "    # Concatenate the outputs\n",
    "    outputs = [out_bowel, out_extra, out_liver, out_kidney, out_spleen]\n",
    "\n",
    "    # Create model\n",
    "    print(\"[INFO] Building the model...\")\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Cosine Decay\n",
    "#     cosine_decay = keras.optimizers.schedules.CosineDecay(\n",
    "#         initial_learning_rate=1e-4,\n",
    "#         decay_steps=decay_steps,\n",
    "#         alpha=0.0,\n",
    "#         warmup_target=1e-3,\n",
    "#         warmup_steps=warmup_steps,\n",
    "#     )\n",
    "    initial_learning_rate = 1e-5\n",
    "    cosine_decay = tf.keras.experimental.CosineDecay(\n",
    "    initial_learning_rate, decay_steps)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=cosine_decay)\n",
    "    loss = {\n",
    "        \"bowel\":keras.losses.BinaryCrossentropy(),\n",
    "        \"extra\":keras.losses.BinaryCrossentropy(),\n",
    "        \"liver\":keras.losses.CategoricalCrossentropy(),\n",
    "        \"kidney\":keras.losses.CategoricalCrossentropy(),\n",
    "        \"spleen\":keras.losses.CategoricalCrossentropy(),\n",
    "    }\n",
    "    metrics = {\n",
    "        \"bowel\":[\"accuracy\"],\n",
    "        \"extra\":[\"accuracy\"],\n",
    "        \"liver\":[\"accuracy\"],\n",
    "        \"kidney\":[\"accuracy\"],\n",
    "        \"spleen\":[\"accuracy\"],\n",
    "    }\n",
    "    print(\"[INFO] Compiling the model...\")\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "      loss=loss,\n",
    "      metrics=metrics\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# %% [markdown]\n",
    "# # Train the model with \"model.fit\"\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:24.929689Z\",\"iopub.execute_input\":\"2023-09-09T19:02:24.930568Z\",\"iopub.status.idle\":\"2023-09-09T19:02:25.036305Z\",\"shell.execute_reply.started\":\"2023-09-09T19:02:24.930528Z\",\"shell.execute_reply\":\"2023-09-09T19:02:25.035235Z\"}}\n",
    "# get image_paths and labels\n",
    "print(\"[INFO] Building the dataset...\")\n",
    "train_paths = train_data.image_path.values; train_labels = train_data[config.TARGET_COLS].values.astype(np.float32)\n",
    "valid_paths = val_data.image_path.values; valid_labels = val_data[config.TARGET_COLS].values.astype(np.float32)\n",
    "\n",
    "# train and valid dataset\n",
    "train_ds = build_dataset(image_paths=train_paths, labels=train_labels)\n",
    "val_ds = build_dataset(image_paths=valid_paths, labels=valid_labels)\n",
    "\n",
    "total_train_steps = train_ds.cardinality().numpy() * config.BATCH_SIZE * config.EPOCHS\n",
    "warmup_steps = int(total_train_steps * 0.10)\n",
    "decay_steps = total_train_steps - warmup_steps\n",
    "\n",
    "print(f\"{total_train_steps=}\")\n",
    "print(f\"{warmup_steps=}\")\n",
    "print(f\"{decay_steps=}\")\n",
    "\n",
    "# %% [code] {\"_kg_hide-input\":true,\"_kg_hide-output\":false,\"jupyter\":{\"outputs_hidden\":false},\"execution\":{\"iopub.status.busy\":\"2023-09-09T19:02:25.040508Z\",\"iopub.execute_input\":\"2023-09-09T19:02:25.040847Z\"}}\n",
    "# build the model\n",
    "print(\"[INFO] Building the model...\")\n",
    "model = build_model(warmup_steps, decay_steps)\n",
    "\n",
    "# train\n",
    "print(\"[INFO] Training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=config.EPOCHS,\n",
    "    validation_data=val_ds,\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Visualize the training plots\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n",
    "# Create a 3x2 grid for the subplots\n",
    "fig, axes = plt.subplots(5, 1, figsize=(5, 15))\n",
    "\n",
    "# Flatten axes to iterate through them\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through the metrics and plot them\n",
    "for i, name in enumerate([\"bowel\", \"extra\", \"kidney\", \"liver\", \"spleen\"]):\n",
    "    # Plot training accuracy\n",
    "    axes[i].plot(history.history[name + '_accuracy'], label='Training ' + name)\n",
    "    # Plot validation accuracy\n",
    "    axes[i].plot(history.history['val_' + name + '_accuracy'], label='Validation ' + name)\n",
    "    axes[i].set_title(name)\n",
    "    axes[i].set_xlabel('Epoch')\n",
    "    axes[i].set_ylabel('Accuracy')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n",
    "plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"val loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n",
    "# store best results\n",
    "best_epoch = np.argmin(history.history['val_loss'])\n",
    "best_loss = history.history['val_loss'][best_epoch]\n",
    "best_acc_bowel = history.history['val_bowel_accuracy'][best_epoch]\n",
    "best_acc_extra = history.history['val_extra_accuracy'][best_epoch]\n",
    "best_acc_liver = history.history['val_liver_accuracy'][best_epoch]\n",
    "best_acc_kidney = history.history['val_kidney_accuracy'][best_epoch]\n",
    "best_acc_spleen = history.history['val_spleen_accuracy'][best_epoch]\n",
    "\n",
    "# Find mean accuracy\n",
    "best_acc = np.mean(\n",
    "    [best_acc_bowel,\n",
    "     best_acc_extra,\n",
    "     best_acc_liver,\n",
    "     best_acc_kidney,\n",
    "     best_acc_spleen\n",
    "])\n",
    "\n",
    "\n",
    "print(f'>>>> BEST Loss  : {best_loss:.3f}\\n>>>> BEST Acc   : {best_acc:.3f}\\n>>>> BEST Epoch : {best_epoch}\\n')\n",
    "print('ORGAN Acc:')\n",
    "print(f'  >>>> {\"Bowel\".ljust(15)} : {best_acc_bowel:.3f}')\n",
    "print(f'  >>>> {\"Extravasation\".ljust(15)} : {best_acc_extra:.3f}')\n",
    "print(f'  >>>> {\"Liver\".ljust(15)} : {best_acc_liver:.3f}')\n",
    "print(f'  >>>> {\"Kidney\".ljust(15)} : {best_acc_kidney:.3f}')\n",
    "print(f'  >>>> {\"Spleen\".ljust(15)} : {best_acc_spleen:.3f}')\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Store the model for inference\n",
    "\n",
    "# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n",
    "# Save the model\n",
    "model.save(\"rsna-arch1_4.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.8_GPU",
   "language": "python",
   "name": "python_3.8_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
